"""
FastAPIä¸»åº”ç”¨å…¥å£
Main FastAPI Application
"""

import logging
import asyncio
import uuid
from datetime import datetime
from typing import Optional
from pathlib import Path

from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

from app.config import settings
from app.models.schemas import PaperInput, PaperAnalysis, AnalysisStatus, TaskResponse
from app.agents.orchestrator import AcademicAnalysisOrchestrator
from app.services.cache_service import CacheService
from app.services.rag_service import RAGService

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.VERSION,
    description="å­¦æœ¯åŠ©æ‰‹ - AIé©±åŠ¨çš„è®ºæ–‡åˆ†æç³»ç»Ÿ"
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€æœåŠ¡å®ä¾‹
orchestrator = AcademicAnalysisOrchestrator()
cache_service = CacheService()
rag_service = RAGService()

# å†…å­˜ä»»åŠ¡é˜Ÿåˆ—
task_queue: dict = {}


@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ å­¦æœ¯åŠ©æ‰‹ç³»ç»Ÿå¯åŠ¨ä¸­...")
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    Path(settings.UPLOAD_DIR).mkdir(parents=True, exist_ok=True)
    Path(settings.CHROMA_PERSIST_DIR).mkdir(parents=True, exist_ok=True)
    Path(settings.LOG_DIR).mkdir(parents=True, exist_ok=True)
    
    # è¿æ¥æœåŠ¡
    await cache_service.connect()
    await rag_service.initialize()
    
    logger.info("âœ… ç³»ç»Ÿå¯åŠ¨å®Œæˆ")


@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    await cache_service.disconnect()
    logger.info("ğŸ‘‹ ç³»ç»Ÿå·²å…³é—­")


@app.get("/")
async def root():
    """æ ¹è·¯ç”± - å¥åº·æ£€æŸ¥"""
    return {
        "service": settings.APP_NAME,
        "version": settings.VERSION,
        "status": "running",
        "endpoints": {
            "analyze": "/api/v1/analyze",
            "status": "/api/v1/status/{task_id}",
            "search": "/api/v1/search",
            "docs": "/docs"
        }
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return {"status": "healthy"}


@app.post("/api/v1/analyze", response_model=TaskResponse)
async def analyze_paper(
    background_tasks: BackgroundTasks,
    file: Optional[UploadFile] = File(None),
    arxiv_id: Optional[str] = None,
    doi: Optional[str] = None,
    title: Optional[str] = None
):
    """
    è®ºæ–‡åˆ†æä¸»æ¥å£
    
    æ”¯æŒä¸‰ç§è¾“å…¥æ–¹å¼ï¼š
    1. ä¸Šä¼ PDFæ–‡ä»¶
    2. æä¾›arXiv ID
    3. æä¾›DOI
    """
    try:
        # ç”Ÿæˆä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = None
        if arxiv_id:
            cache_key = f"paper:arxiv:{arxiv_id}"
        elif doi:
            cache_key = f"paper:doi:{doi}"
        
        if cache_key and settings.ENABLE_REDIS_CACHE:
            cached_result = await cache_service.get(cache_key)
            if cached_result:
                logger.info(f"ç¼“å­˜å‘½ä¸­: {cache_key}")
                return TaskResponse(
                    task_id=task_id,
                    status=AnalysisStatus.COMPLETED,
                    result=PaperAnalysis(**cached_result)
                )
        
        # éªŒè¯è¾“å…¥
        if not file and not arxiv_id and not doi:
            raise HTTPException(
                status_code=400,
                detail="å¿…é¡»æä¾›PDFæ–‡ä»¶ã€arXiv IDæˆ–DOIä¹‹ä¸€"
            )
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file_path = None
        if file:
            file_path = f"{settings.UPLOAD_DIR}/{task_id}_{file.filename}"
            with open(file_path, "wb") as f:
                content = await file.read()
                f.write(content)
        
        # åˆ›å»ºä»»åŠ¡
        paper_input = PaperInput(
            file_path=file_path,
            arxiv_id=arxiv_id,
            doi=doi,
            title=title
        )
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        task_queue[task_id] = {
            "status": AnalysisStatus.PENDING,
            "progress": 0,
            "result": None,
            "error": None,
            "created_at": datetime.now()
        }
        
        # åå°æ‰§è¡Œåˆ†æ
        background_tasks.add_task(
            run_analysis,
            task_id,
            paper_input,
            cache_key
        )
        
        return TaskResponse(
            task_id=task_id,
            status=AnalysisStatus.PROCESSING,
            message=f"åˆ†æå·²å¯åŠ¨ï¼Œä½¿ç”¨ /api/v1/status/{task_id} æŸ¥è¯¢è¿›åº¦"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ†ææ¥å£é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


async def run_analysis(task_id: str, paper_input: PaperInput, cache_key: Optional[str]):
    """åå°æ‰§è¡Œè®ºæ–‡åˆ†æ"""
    try:
        logger.info(f"[{task_id}] å¼€å§‹åˆ†æ")
        task_queue[task_id]["status"] = AnalysisStatus.PROCESSING
        task_queue[task_id]["progress"] = 10
        
        # æ‰§è¡Œåˆ†æ
        result = await orchestrator.analyze_paper(paper_input)
        
        task_queue[task_id]["progress"] = 100
        task_queue[task_id]["status"] = AnalysisStatus.COMPLETED
        task_queue[task_id]["result"] = result.dict()
        
        # ç¼“å­˜ç»“æœ
        if cache_key and settings.ENABLE_REDIS_CACHE:
            await cache_service.set(cache_key, result.dict())
        
        logger.info(f"[{task_id}] åˆ†æå®Œæˆ")
        
    except Exception as e:
        logger.error(f"[{task_id}] åˆ†æå¤±è´¥: {str(e)}")
        task_queue[task_id]["status"] = AnalysisStatus.FAILED
        task_queue[task_id]["error"] = str(e)


@app.get("/api/v1/status/{task_id}", response_model=TaskResponse)
async def get_task_status(task_id: str):
    """æŸ¥è¯¢ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€"""
    if task_id not in task_queue:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    
    task = task_queue[task_id]
    
    return TaskResponse(
        task_id=task_id,
        status=task["status"],
        progress=task["progress"],
        result=PaperAnalysis(**task["result"]) if task["result"] else None,
        error=task["error"]
    )


@app.get("/api/v1/search")
async def search_papers(query: str, limit: int = 10):
    """
    åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸ä¼¼è®ºæ–‡
    """
    try:
        if limit > 50:
            limit = 50
        
        results = await rag_service.search(query, limit=limit)
        
        return {
            "query": query,
            "results": results,
            "count": len(results)
        }
    except Exception as e:
        logger.error(f"æœç´¢é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/v1/metrics")
async def get_metrics():
    """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
    total_tasks = len(task_queue)
    completed = sum(1 for t in task_queue.values() if t["status"] == AnalysisStatus.COMPLETED)
    failed = sum(1 for t in task_queue.values() if t["status"] == AnalysisStatus.FAILED)
    
    return {
        "total_tasks": total_tasks,
        "completed_tasks": completed,
        "failed_tasks": failed,
        "success_rate": completed / max(total_tasks, 1),
        "timestamp": datetime.now().isoformat()
    }


if __name__ == "__main__":
    uvicorn.run(
        app,
        host=settings.HOST,
        port=settings.PORT,
        workers=settings.WORKERS,
        reload=settings.RELOAD,
        log_level=settings.LOG_LEVEL.lower()
    )
